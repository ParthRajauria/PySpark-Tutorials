{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\6thenergy\\anaconda3\\envs\\streamlit_demo\\lib\\site-packages\\pyspark\\context.py:238: FutureWarning: Python 3.6 support is deprecated in Spark 3.2.\n",
      "  FutureWarning\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession         # This is how we start spark session.\n",
    "spark = SparkSession.builder.appName('MachineLearningExample').getOrCreate()      # Starting the session with app name 'Practise'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read the dataset\n",
    "training = spark.read.csv('water.csv', header = True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|Year|Water|\n",
      "+----+-----+\n",
      "|1885|  356|\n",
      "|1886|  386|\n",
      "|1887|  397|\n",
      "|1888|  397|\n",
      "|1889|  413|\n",
      "|1890|  458|\n",
      "|1891|  485|\n",
      "|1892|  344|\n",
      "|1893|  390|\n",
      "|1894|  360|\n",
      "|1895|  420|\n",
      "|1896|  435|\n",
      "|1897|  439|\n",
      "|1898|  454|\n",
      "|1899|  462|\n",
      "|1900|  454|\n",
      "|1901|  469|\n",
      "|1902|  500|\n",
      "|1903|  492|\n",
      "|1904|  473|\n",
      "+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Water: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Year', 'Water']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In Pandas we split the data into train-test split for training and model validation but in Pyspark, we group all our independent features, by using Vector Assembler. \n",
    "* [\"Year\"] ----> new feature ----> independent feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for vector assembler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "featureassembler1 = VectorAssembler(inputCols = [\"Year\"],outputCol = \"Independent_Year\")\n",
    "featureassembler2 = VectorAssembler(inputCols = [\"Year\", \"Water\"],outputCol = \"Independent_Year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = featureassembler1.transform(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------------+\n",
      "|Year|Water|Independent_Year|\n",
      "+----+-----+----------------+\n",
      "|1885|  356|        [1885.0]|\n",
      "|1886|  386|        [1886.0]|\n",
      "|1887|  397|        [1887.0]|\n",
      "|1888|  397|        [1888.0]|\n",
      "|1889|  413|        [1889.0]|\n",
      "|1890|  458|        [1890.0]|\n",
      "|1891|  485|        [1891.0]|\n",
      "|1892|  344|        [1892.0]|\n",
      "|1893|  390|        [1893.0]|\n",
      "|1894|  360|        [1894.0]|\n",
      "|1895|  420|        [1895.0]|\n",
      "|1896|  435|        [1896.0]|\n",
      "|1897|  439|        [1897.0]|\n",
      "|1898|  454|        [1898.0]|\n",
      "|1899|  462|        [1899.0]|\n",
      "|1900|  454|        [1900.0]|\n",
      "|1901|  469|        [1901.0]|\n",
      "|1902|  500|        [1902.0]|\n",
      "|1903|  492|        [1903.0]|\n",
      "|1904|  473|        [1904.0]|\n",
      "+----+-----+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Year', 'Water', 'Independent_Year']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalized_data = output.select(\"Independent_Year\", \"Water\")    # Water is my target feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|Independent_Year|Water|\n",
      "+----------------+-----+\n",
      "|        [1885.0]|  356|\n",
      "|        [1886.0]|  386|\n",
      "|        [1887.0]|  397|\n",
      "|        [1888.0]|  397|\n",
      "|        [1889.0]|  413|\n",
      "|        [1890.0]|  458|\n",
      "|        [1891.0]|  485|\n",
      "|        [1892.0]|  344|\n",
      "|        [1893.0]|  390|\n",
      "|        [1894.0]|  360|\n",
      "|        [1895.0]|  420|\n",
      "|        [1896.0]|  435|\n",
      "|        [1897.0]|  439|\n",
      "|        [1898.0]|  454|\n",
      "|        [1899.0]|  462|\n",
      "|        [1900.0]|  454|\n",
      "|        [1901.0]|  469|\n",
      "|        [1902.0]|  500|\n",
      "|        [1903.0]|  492|\n",
      "|        [1904.0]|  473|\n",
      "+----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finalized_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For doing train test split and Linear Regression training\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "train_data, test_data = finalized_data.randomSplit([0.75,0.25])   # train-test split\n",
    "regressor = LinearRegression(featuresCol = \"Independent_Year\", labelCol = \"Water\")    # Defining linear regression model.\n",
    "regressor = regressor.fit(train_data)   # fitting the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([2.6598])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Coefficients\n",
    "regressor.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4602.720425348935"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Intercepts\n",
    "regressor.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prediction\n",
    "pred_results = regressor.evaluate(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\6thenergy\\anaconda3\\envs\\streamlit_demo\\lib\\site-packages\\pyspark\\sql\\context.py:127: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+------------------+\n",
      "|Independent_Year|Water|        prediction|\n",
      "+----------------+-----+------------------+\n",
      "|        [1886.0]|  386|413.57904994533783|\n",
      "|        [1891.0]|  485| 426.8778290420887|\n",
      "|        [1900.0]|  454|450.81563141623883|\n",
      "|        [1906.0]|  469| 466.7741663323395|\n",
      "|        [1909.0]|  466|474.75343379038986|\n",
      "|        [1917.0]|  526| 496.0314803451911|\n",
      "|        [1926.0]|  450| 519.9692827193412|\n",
      "|        [1929.0]|  458| 527.9485501773916|\n",
      "|        [1933.0]|  466| 538.5875734547926|\n",
      "|        [1948.0]|  613| 578.4839107450434|\n",
      "|        [1950.0]|  575|  583.803422383744|\n",
      "|        [1954.0]|  568| 594.4424456611441|\n",
      "|        [1955.0]|  575| 597.1022014804939|\n",
      "|        [1957.0]|  587| 602.4217131191945|\n",
      "|        [1959.0]|  594|  607.741224757895|\n",
      "+----------------+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_results.predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Databricks is a platform where we can use pyspark and apache spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
